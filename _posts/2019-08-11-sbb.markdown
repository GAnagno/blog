---
layout: post
title:  "SBB"
date:   2019-08-11 09:00:00 +0200
---
In the [previous post][previous], the graph consisted of 25 nodes. Who needs computers, right? `SBB`, the Swiss Federal Railways, definitely do. In fact, they maintain an amazing data repository comprised of over sixty `Datasets` with many thousand entries each. The company even launched a `Kaggle` competition in 2018 because there was no standard software to cover their increasing needs.  

Here I would like to show the bigger picture without going into task-specific problem solving. It is about connecting the dots, making a living organism out of the network, beyond coordinate systems.  

# Basel SBB
![Basel SBB](https://github.com/GAnagno/myblog/blob/gh-pages/assets/images/Uhr.jpg?raw=true)

{% highlight python %}
%matplotlib inline
import os
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from matplotlib import cm
{% endhighlight %}

# Import Data
The original dataset contains information about the yearly total number of trains that pass through each route section of Switzerland. The list contains entries for 2016, 2017 and 2018. Distinction is also made between passenger transport and cargo. For this post, I will focus only on passenger transport in 2018. Comparative studies along the aforementioned two axes are reserved for the near future.

{% highlight python %}
df = pd.read_csv('data/zugzahlen.csv')
{% endhighlight %}

| PID          | Anzahl_Zuege |
| ------------ | ------------ |
| SBB_GESE_CHY | 126307       |
| SBB_GIBU_ROL | 97943        |
| SBB_MIES_TAN | 126101       |
| SBB_MOR_STJ  | 121566       |
| SBB_PER_ALL  | 98005        |

# Preprocessing
As is often the case with transportation networks, there are origin and destination points. Caution :skull:! These two sets of points do not have to be congruent and in this case they aren't. Let's extract the respective longitudes and latitudes in order to have a first picture of the distribution.

#### Longitude of origin

{% highlight python %}
df['lon_von'] = df.geopos_von.str.split('\,').str[0]
df['lon_von'] = df['lon_von'].map(float)
{% endhighlight %}

#### Latitude of origin

{% highlight python %}
df['lat_von'] = df.geopos_von.str.split('\,').str[1]
df['lat_von'] = df['lat_von'].map(float)
{% endhighlight %}

#### Longitude of destination

{% highlight python %}
df['lon_bis'] = df.geopos_bis.str.split('\,').str[0]
df['lon_bis'] = df['lon_bis'].map(float)
{% endhighlight %}

#### Latitude of destination

{% highlight python %}
df['lat_bis'] = df.geopos_bis.str.split('\,').str[1]
df['lat_bis'] = df['lat_bis'].map(float)
{% endhighlight %}

#### Map of origins and destinations

Spot the difference :trollface:

![Map](https://github.com/GAnagno/myblog/blob/gh-pages/assets/images/VonBis.jpg?raw=true)

# Graph construction

Now it is time to build the network. The graph is small and should be easy to construct but there is a problem. 'Open' and 'Sales Arena' are connected with 2 doors. A double edge or `Multi-edge` is allowed but hinders calculations and for all intends and purposes can be replaced by a weight.

{% highlight python %}
fr = np.array(doors['From Room'])
to = np.array(doors['To Room'])
pairs = np.vstack((fr, to)).T
{% endhighlight %}

{% highlight python %}

import warnings
warnings.filterwarnings('ignore')
 
# Create a networkx MultiGraph object
G = nx.MultiGraph() 
 
# Add edges to to the graph object
# Each tuple represents an edge between two nodes
G.add_edges_from(pairs[:, [0, 1]])
c = np.array(G.edges).T[2].astype(int)
my_pos = nx.spring_layout(G, seed=56)
# Draw the resulting graph
nx.draw(G, pos=my_pos, with_labels=True, edge_color=c, width=1.5, node_size=4, edge_cmap=plt.cm.Paired)
{% endhighlight %}

![MultiGraph](https://github.com/GAnagno/myblog/blob/gh-pages/assets/images/MultiGraph.png?raw=true)

# Embed Attributes

At the moment our MulitiGraph is not attributed. It needs to be converted into a simple weighted Graph with embedded node attributes.

{% highlight python %}
weight = 0
nx.set_edge_attributes(G, weight, 'weight')
{% endhighlight %}

{% highlight python %}
for i, pair in enumerate(pairs):
    G.edges[(pair[0], pair[1], pair[2])]['weight'] = bond[i]
{% endhighlight %}

{% highlight python %}
# Weighted Graph N from MultiGraph G
N = nx.Graph()
for u,v,data in G.edges(data=True):
    w = data['weight'] if 'weight' in data else 1.0
    if N.has_edge(u,v):
        N[u][v]['weight'] += w
    else:
        N.add_edge(u, v, weight=w)
{% endhighlight %}

{% highlight python %}
for i, node in enumerate(rooms['Name']):
    N.nodes[node]['area'] = ra[i]
{% endhighlight %}

#### The Graph rendered as a Matrix

{% highlight python %}
# Graph to matrix 
A = nx.to_numpy_matrix(N)
plt.figure(figsize=(15,8))
plt.title('Adjacency matrix')
sns.heatmap(A, cmap=plt.cm.Paired, annot=True, xticklabels=N.nodes, yticklabels=N.nodes);
{% endhighlight %}

![Matrix](https://github.com/GAnagno/myblog/blob/gh-pages/assets/images/Matrix.png?raw=true)

# Gephi Export

Why is a graph useful in the first place? Because it is operational, not just a visualization. It can provide us with network-specific metrics that add to the dimensionality of the initial dataset. For a first evaluation, `Gephi`, an open-source graph-editing software, is a good option.

{% highlight python %}
with open('data/room_graph.graphml', 'wb') as ofile:
    nx.write_graphml(N, ofile)
{% endhighlight %}

#### Get data laboratory results

{% highlight python %}
# Load data from Gephi
data = pd.read_csv('data/data_lab.csv')
data = data.rename(columns={'d0': 'area'})
columns = ['Id', 'timeset', 'componentnumber']
data.drop(columns, inplace=True, axis=1)
{% endhighlight %}

| Label              | eigencentrality |
| ------------------ | --------------- |
| CTO Office         | 0.209728        |
| Open               | 1.000000        |
| Legal Eagle Office | 0.209728        |
| PA Office          | 0.209728        |
| CEO Office         | 0.209728        |

This method effectively expands the dataset from two to fourteen dimensions.

{% highlight python %}
labels = np.array(data['Label'])
X = np.array(data.drop(['Label'], axis=1))
X.shape
{% endhighlight %}

(25, 14)

# Clustering

Our data are sparse to begin with and clustering in high dimensions is destined to fail. In order to escape the `curse-of-dimensionality`, an intermediate mapping is needed.

#### Step 1: `Non-linear` dimensionality reduction.

{% highlight python %}
# Apply isomap with k = 24 and output dimension = 3
from sklearn.manifold import Isomap
model = Isomap(n_components=3, n_neighbors=24)
proj = model.fit_transform(X)
{% endhighlight %}

#### Step 2: `Kmeans` clustering in 3 dimensions.

{% highlight python %}
# Apply deterministic KMeans with n_clusters = 7
from sklearn.cluster import KMeans
Kmean = KMeans(n_clusters=7, random_state=13)
Kmean.fit(proj)
{% endhighlight %}

{% highlight python %}
# Plot clustered isomap
fig = plt.figure(figsize=(14,10))
ax = fig.add_subplot(1, 1, 1)
ax.scatter(proj[:, 0], proj[:, 1], cmap=cmap, c=clusters, s=140, lw=0, alpha=1);
# Remove the top and right axes from the data plot
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.set_title('Kmeans clustering on projected data', fontsize=16);
for i, m in enumerate(match):
    ax.text(proj[i, 0]+0.07, proj[i, 1]+0.07, m[-1], fontsize=12)
{% endhighlight %}

![Kmeans](https://github.com/GAnagno/myblog/blob/gh-pages/assets/images/Kmeans.png?raw=true)

The algorithm successfully identifies meaningful room clusters.

{% highlight python %}
img = mpimg.imread('data/clusters.jpg')
fig = plt.figure(figsize = (30,10))
ax = fig.add_subplot(1, 1, 1)
ax.imshow(img, interpolation='bilinear')
ax.axis('off')
ax.set_title('HQ Clusters', fontsize=16);
{% endhighlight %}

![HQ clusters](https://github.com/GAnagno/myblog/blob/gh-pages/assets/images/HQclusters.png?raw=true)

![HQ graph](https://github.com/GAnagno/myblog/blob/gh-pages/assets/images/HQgraph.png?raw=true)

Check out the [Jupyter notebook][notebook] for the full code.

[notebook]: https://github.com/GAnagno/Social-Web/blob/master/SBB.ipynb
[previous]: https://ganagno.github.io/myblog/2019/08/09/room-graph.html
